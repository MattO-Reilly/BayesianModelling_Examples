---
title: "Bayesian_Project"
author: Matt O'Reilly and Kordian Pawelec
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

*Libraries*
```{r}
library(tidyverse)
library(ggExtra)
library(ggplot2)
source('TeachBayes.r')
library(rjags)
library(ggpubr)
```

```{r}
#Load data from Github
math_student_data_url <- "https://raw.githubusercontent.com/arunk13/MSDA-Assignments/master/IS607Fall2015/Assignment3/student-mat.csv";
math_student_data <- read.table(file = math_student_data_url, header = TRUE, sep = ";");
eng_student_data_url <- "https://raw.githubusercontent.com/arunk13/MSDA-Assignments/master/IS607Fall2015/Assignment3/student-por.csv";
eng_student_data <- read.table(file = eng_student_data_url, header = TRUE, sep = ";")
studentdata <- rbind(math_student_data, eng_student_data)


#Clean Data
studentdata[c("G1","G2","G3")] <- studentdata[c("G1","G2","G3")] * 5

```
Grades were originally out of 20 (Based on european grading scale) but for Ireland we would like our results out of 100 so we have multiplied our grades by 5 to achieve our desired results.

```{r}
studentdata %>% summarise(N=n(), mean = mean(studentdata$G3), sd = sd(studentdata$G3))
```
There are N=1044 students in our dataset, The mean final grade is 56.70977 while the standard deviation is 19.32398. With normal data, most of the observations are spread within 3 standard deviations on each side of the mean.


*Defining prior*
```{r}
m0 <- 60
se <- 19.32398/sqrt(1044)
s0 <- 1 #Sigma_0 (Using a larger value for sigma_0 to allow for extra variation in Final Grade)
```

*Likelihood*
```{r}
xbar <- mean(studentdata$G3)
sigma <- 15
n = length(studentdata$G3)
se <- sigma/sqrt(n)

xbar
```
*Plot the prior, data and posterior*
```{r}
Prior <- c(m0, s0)
Data <- c(xbar, se)
Posterior <- round(normal_update(Prior, Data),3)
x <- seq(0, 100, length=1000)
priorx <- dnorm(x, mean=m0,   sd=s0)
datax  <- dnorm(x, mean=xbar, sd=se)
postx  <- dnorm(x, mean=Posterior[1], sd=Posterior[2])
plot(x, priorx, type='l',lwd=3,xlim = c(50,65),ylim=c(0,1),col = 'blue', main = '', xlab = 'theta', ylab = '')
lines(x, datax,col='black',lwd=3)
lines(x, postx,col='red',lwd=3)
legend("topright", c("Prior","Data","Posterior"), lty = 1, lwd= 3, col = c('blue','black','red'))
```
The posterior has shifted to the left towards the data. Our prior belief is a normal distribution $X\sim N(\theta,\sigma^2)$ with mean = 60 and $\sigma$ = 1. The mode is now approximately 57. We can see the distribution is more precise than both the data and prior, taking advatange of combining information from both.

*Measures of spread*
```{r}
mysims <- rnorm(10000, mean = Posterior[1], sd = Posterior[2])
diff(quantile(mysims, probs = c(0.25,0.75)))
```


*Comparing Grades by Sex*
```{r}
studentdata %>% group_by(sex) %>% summarise(mean = mean(G3), n = length(sex), sigma = sd(G3))

n_F <- sum(studentdata$sex=='F')
n_M <- sum(studentdata$sex=='M')

#se_male <- sigma/sqrt(n_F)
#se_female <- sigma/sqrt(n_M)
```
Summarising the mean grade by sex we can see that females have a mean grade slightly higher than males. However, the difference is quite small.

```{r}
xbar1_F <- studentdata %>% filter(studentdata$sex == 'F') %>% 
  summarise(mean(G3)) %>% as.numeric()
xbar2_M <- studentdata %>% filter(studentdata$sex == 'M') %>% 
  summarise(mean(G3)) %>% as.numeric()
Prior <- c(m0, s0)
Grades_Female <- c(xbar1_F, se)
Grades_Male <- c(xbar2_M, se)
Posterior1_F <- normal_update(Prior, Grades_Female)

Posterior2_M <- normal_update(Prior, Grades_Male)

many_normal_plots(list(Prior,Posterior1_F,Posterior2_M)) + theme(legend.position = c(0.75,0.75))
```
From the plot, we can see that the Blue curve is our prior. The red curve represents the average final grade of male students, which has a mode of 57.0651757. The green distribution represents the average final grade of female students, which has a mode of 57.9685636.

#Estimate Mean and SD using rjags
```{r}
# Take 10000 samples from the theta prior
prior_m <- rnorm(10000, 56.7, 15)
# Take 10000 samples from the sigma prior 
prior_s <- runif(10000, 0, 50)
samples <- data.frame(prior_m, prior_s)

grade_model <- "model{
  #Likelihood model for X
  for(i in 1:length(X)) {
      X[i] ~ dnorm(theta, sigma^(-2))
  }

  # Prior models for theta and sigma
  theta ~ dnorm(56.7, 15^(-2))
  sigma ~ dunif(0, 50)
}"

grade_jags <- jags.model(textConnection(grade_model), data = list(X = studentdata$G3),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989))

grade_sim <- update(grade_jags, n.iter = 10000)    
# SIMULATE the posterior  
n = 20000
grade_sim <- coda.samples(model = grade_jags, 
                         variable.names = c("theta","sigma"), 
                         n.iter= n)
head(grade_sim)


# PLOT the posterior    
plot(grade_sim, density = FALSE)

# PLOT the posterior    
plot(grade_sim, trace = FALSE)

# Store the chains in a data frame
grade_chains <- data.frame(sim = 1:n, grade_sim[[1]])
# Check out the head of rent_chains
head(grade_chains)

mean(grade_chains$sigma)
mean(grade_chains$theta)
```
We can see the trace plots for Sigma and Theta have no patterns and there is good mixing this indicates that they are stable.

The density plots are also normally distributed  

#Running multiple chains
```{r}
# COMPILE the model
grade_jags_multi <- jags.model(textConnection(grade_model), 
                              data = list(X = studentdata$G3), 
                              n.chains = 4)   
# UPDATE the model    
grade_sim_multi <- update(grade_jags_multi,
                         n.iter = 10000)
# SIMULATE the posterior    
grade_sim_multi <- coda.samples(model = grade_jags_multi, 
                               variable.names = c("theta", "sigma"), 
                               n.iter = 20000)
head(grade_sim_multi)

# Construct trace plots of the theta and sigma chains
plot(grade_sim_multi, density = FALSE)

# Construct density plots of the theta and sigma chains
plot(grade_sim_multi, trace = FALSE)

grade_chains_multi <- data.frame(sim = rep(1:20000,4),
                                chain = c(rep(1,20000),
                                          rep(2,20000),
                                          rep(3,20000),
                                          rep(4,20000)),
                                rbind(grade_sim_multi[[1]],
                                      grade_sim_multi[[2]],
                                      grade_sim_multi[[3]],
                                      grade_sim_multi[[4]]))

grade_chains_multi %>% filter(sim<1000) %>%
ggplot(aes(x=sim,y=theta,color=as.factor(chain))) + geom_line() + 
  geom_smooth(aes(color=as.factor(chain)),se=FALSE)
```
**Gelman-Rubin diagnostic**
```{r}
gelman.diag(grade_sim_multi)
gelman.plot(grade_sim_multi)
```
Since the median converges to 1 quite quickly, this suggests the chains have converged to the stationary distribution

```{r}
autocorr.plot(grade_sim)
```
Here there is no correlation between the iterates of \(theta\), while there is minimal correlation for \(sigma\). 



--------------------------------------------


**Time study and grades**
```{r}
time_model <- "model{
    # Likelihood model for Y[i]
    for(i in 1:length(Y)) {
      Y[i] ~ dpois(theta[i])
      log(theta[i]) <- a + b*X[i] # log-link
    }

    # Define the flat priors for a, b
    a ~ dnorm(0, 19^(-2))
    b ~ dnorm(0, 19^(-2))
    
    b.counts <- exp(b)

  
}"


time_jags <- jags.model(textConnection(time_model), 
                        data = list(Y = studentdata$G3, 
                                    X = studentdata$studytime), 
                        n.chains = 4)





# BURN IN the model
time_sim <- update(time_jags, n.iter = 10000)
# SIMULATE the posterior    
time_sim <- coda.samples(model=time_jags,
                         variable.names=c("a","b","b.counts"),
                         n.iter=10000,
                         thin = 10)
```








```{r}
plot(time_sim[[1]][,1], main = 'a')
plot(time_sim[[1]][,2], main = 'b')
plot(time_sim[[1]][,3], main = 'b.counts')
```







```{r}
summary(time_sim)
```



```{r}
freq_poisson_model <- glm(studytime ~ G3, family = poisson(link = "log"), data = studentdata)
summary(freq_poisson_model)  
```


```{r}
gelman.diag(time_sim)
gelman.plot(time_sim[,1:3])
autocorr.plot(time_sim[[1]][,1:3])
```

```{r}
time_chains <- data.frame(time_sim[[1]])
quantile(time_chains$b.counts, probs = c(0.025,0.975))


ggplot(time_chains, aes(x = b.counts)) + geom_density() + 
  geom_vline(xintercept = quantile(time_chains$b.counts, probs = c(0.025,0.975)), color = 'red')
```
There is a positive association between time studied and grade achieved
For every hour of study, mean grade achieved increases by 7.8%
There's a probability of 0.95 that study time increases mean grade achieved  by between 5.6, 7.8%
