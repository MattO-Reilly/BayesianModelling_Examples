---
title: "Bayesian_Project"
author: Matt O'Reilly and Kordian Pawelec
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

*Libraries*
```{r}
library(tidyverse)
library(ggExtra)
library(ggplot2)
source('TeachBayes.r')
library(rjags)
library(ggpubr)
```

###Aim
2020 marked the first-ever occurence of predicted grades for students in Ireland. The Irish Government had to provide an estimated mark across all subjects for each student in the country. There were many issues with their model.So, Using Bayesian Statistics and a dataset of student grades, we want to build a model that can predict an end of year grade from personal and academic characteristics.

Our objective is to create a model that can acuurately predict end of year grades based on student information. We will aim to use variables like Mock Examination Results, Gender, Study Time, Absences, Failures, and Parents Education to predict their end of year grade. 

###Subjective Impression

We found a dataset online \href{https://archive.ics.uci.edu/ml/datasets/Student+Performance}{here}. The dataset included grades and student information for 1044 students taken from a school in Portugal. The portuguese system measures grades from 0 to 20 and for convenience we have multiplied the grades by 5 to get a grade out of 100.  
```{r}
#Load data from Github
math_student_data_url <- "https://raw.githubusercontent.com/arunk13/MSDA-Assignments/master/IS607Fall2015/Assignment3/student-mat.csv";
math_student_data <- read.table(file = math_student_data_url, header = TRUE, sep = ";");
eng_student_data_url <- "https://raw.githubusercontent.com/arunk13/MSDA-Assignments/master/IS607Fall2015/Assignment3/student-por.csv";
eng_student_data <- read.table(file = eng_student_data_url, header = TRUE, sep = ";")
studentdata <- rbind(math_student_data, eng_student_data)


#Clean Data
studentdata[c("G1","G2","G3")] <- studentdata[c("G1","G2","G3")] * 5

```

##Estimate Mean Grade
```{r}
studentdata %>% summarise(N=n(), mean = mean(studentdata$G3), sd = sd(studentdata$G3))
```
There are N=1044 students in our dataset, The mean final grade is 56.70977 while the standard deviation is 19.32398. With normal data, most of the observations are spread within 3 standard deviations on each side of the mean.


*Defining prior*
```{r}
m0 <- 60
se <- 19.32398/sqrt(1044)
s0 <- 1 #Sigma_0 (Using a larger value for sigma_0 to allow for extra variation in Final Grade)
```

*Likelihood*
```{r}
xbar <- mean(studentdata$G3)
sigma <- 15
n = length(studentdata$G3)
se <- sigma/sqrt(n)

xbar
```
*Plot the prior, data and posterior*
```{r}
Prior <- c(m0, s0)
Data <- c(xbar, se)
Posterior <- round(normal_update(Prior, Data),3)
x <- seq(0, 100, length=1000)
priorx <- dnorm(x, mean=m0,   sd=s0)
datax  <- dnorm(x, mean=xbar, sd=se)
postx  <- dnorm(x, mean=Posterior[1], sd=Posterior[2])
plot(x, priorx, type='l',lwd=3,xlim = c(50,65),ylim=c(0,1),col = 'blue', main = '', xlab = 'theta', ylab = '')
lines(x, datax,col='black',lwd=3)
lines(x, postx,col='red',lwd=3)
legend("topright", c("Prior","Data","Posterior"), lty = 1, lwd= 3, col = c('blue','black','red'))
```
The posterior has shifted to the left towards the data. Our prior belief is a normal distribution $X\sim N(\theta,\sigma^2)$ with mean = 60 and $\sigma$ = 1. The mode is now approximately 57. We can see the distribution is more precise than both the data and prior, taking advatange of combining information from both.

*Measures of spread*
```{r}
mysims <- rnorm(10000, mean = Posterior[1], sd = Posterior[2])
diff(quantile(mysims, probs = c(0.25,0.75)))
```


##Comparing Grades by Sex
```{r}
studentdata %>% group_by(sex) %>% summarise(mean = mean(G3), n = length(sex), sigma = sd(G3))

n_F <- sum(studentdata$sex=='F')
n_M <- sum(studentdata$sex=='M')

#se_male <- sigma/sqrt(n_F)
#se_female <- sigma/sqrt(n_M)
```
Summarising the mean grade by sex we can see that females have a mean grade slightly higher than males. However, the difference is quite small.

```{r}
xbar1_F <- studentdata %>% filter(studentdata$sex == 'F') %>% 
  summarise(mean(G3)) %>% as.numeric()
xbar2_M <- studentdata %>% filter(studentdata$sex == 'M') %>% 
  summarise(mean(G3)) %>% as.numeric()
Prior <- c(m0, s0)
Grades_Female <- c(xbar1_F, se)
Grades_Male <- c(xbar2_M, se)
Posterior1_F <- normal_update(Prior, Grades_Female)

Posterior2_M <- normal_update(Prior, Grades_Male)

many_normal_plots(list(Prior,Posterior1_F,Posterior2_M)) + theme(legend.position = c(0.75,0.75))
```
From the plot, we can see that the Blue curve is our prior. The red curve represents the average final grade of male students, which has a mode of 57.0651757. The green distribution represents the average final grade of female students, which has a mode of 57.9685636.

##Estimate Mean and SD using rjags
```{r}
# Take 10000 samples from the theta prior
prior_m <- rnorm(10000, 56.7, 15)
# Take 10000 samples from the sigma prior 
prior_s <- runif(10000, 0, 50)
samples <- data.frame(prior_m, prior_s)

grade_model <- "model{
  #Likelihood model for X
  for(i in 1:length(X)) {
      X[i] ~ dnorm(theta, sigma^(-2))
  }

  # Prior models for theta and sigma
  theta ~ dnorm(56.7, 15^(-2))
  sigma ~ dunif(0, 50)
}"

grade_jags <- jags.model(textConnection(grade_model), data = list(X = studentdata$G3),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989))

grade_sim <- update(grade_jags, n.iter = 10000)    
# SIMULATE the posterior  
n = 20000
grade_sim <- coda.samples(model = grade_jags, 
                         variable.names = c("theta","sigma"), 
                         n.iter= n)
head(grade_sim)


# PLOT the posterior    
plot(grade_sim, density = FALSE)

# PLOT the posterior    
plot(grade_sim, trace = FALSE)

# Store the chains in a data frame
grade_chains <- data.frame(sim = 1:n, grade_sim[[1]])
# Check out the head of rent_chains
head(grade_chains)

mean(grade_chains$sigma)
mean(grade_chains$theta)
```
It is desirable to have stability in the trace plot (no patterns), with good mixing. From our trace plots for Sigma and Theta we can see they have no patterns and there is good mixing, this indicates that they are stable.

The density plots are also normally distributed  

#Running multiple chains
```{r}
# COMPILE the model
grade_jags_multi <- jags.model(textConnection(grade_model), 
                              data = list(X = studentdata$G3), 
                              n.chains = 4)   
# UPDATE the model    
grade_sim_multi <- update(grade_jags_multi,
                         n.iter = 10000)
# SIMULATE the posterior    
grade_sim_multi <- coda.samples(model = grade_jags_multi, 
                               variable.names = c("theta", "sigma"), 
                               n.iter = 20000)
head(grade_sim_multi)

# Construct trace plots of the theta and sigma chains
plot(grade_sim_multi, density = FALSE)

# Construct density plots of the theta and sigma chains
plot(grade_sim_multi, trace = FALSE)

grade_chains_multi <- data.frame(sim = rep(1:20000,4),
                                chain = c(rep(1,20000),
                                          rep(2,20000),
                                          rep(3,20000),
                                          rep(4,20000)),
                                rbind(grade_sim_multi[[1]],
                                      grade_sim_multi[[2]],
                                      grade_sim_multi[[3]],
                                      grade_sim_multi[[4]]))

grade_chains_multi %>% filter(sim<1000) %>%
ggplot(aes(x=sim,y=theta,color=as.factor(chain))) + geom_line() + 
  geom_smooth(aes(color=as.factor(chain)),se=FALSE)
```
**Gelman-Rubin diagnostic**
```{r}
gelman.diag(grade_sim_multi)
gelman.plot(grade_sim_multi)
```
Since the median converges to 1 quite quickly, this suggests the chains have converged to the stationary distribution

```{r}
autocorr.plot(grade_sim)
```
Here there is no correlation between the iterates of \(theta\), while there is minimal correlation for \(sigma\). 


##Bayesian Regression (Time spent studying and grades)
```{r}
studytime_model <- "model{
    # Likelihood model for Y[i]
    for(i in 1:length(Y)) {
      m[i] <- a + b * X[i]
      Y[i] ~ dnorm(m[i], s^(-2))
    }

    # Define the a, b, s priors
    a ~ dnorm(0, 19^(-2))
    b ~ dnorm(0, 19^(-2))
    s ~ dunif(0, 30)}"

studytime_jags <- jags.model(textConnection(studytime_model), 
                        data = list(Y = studentdata$G3, 
                                    X = studentdata$studytime), 
                        n.chains = 4)

# BURN IN the model
studytime_sim <- update(studytime_jags, n.iter = 10000)
# SIMULATE the posterior    
studytime_sim <- coda.samples(model=studytime_jags,
                         variable.names=c("a","b","s"),
                         n.iter=20000,
                         thin = 10)
```


```{r}
plot(studytime_sim[[1]][,1], main = 'a')
plot(studytime_sim[[1]][,2], main = 'b')
plot(studytime_sim[[1]][,3], main = 's')
```


```{r}
summary(studytime_sim)
```

```{r}
#freq_poisson_model <- glm(studytime ~ G3, family = poisson(link = "log"), data = studentdata)
#summary(freq_poisson_model)  
```

```{r}
gelman.diag(studytime_sim)
gelman.plot(studytime_sim[,1:3])
autocorr.plot(studytime_sim[[1]][,1:3])
```
Since the median converges to 1 quite quickly, this suggests the chains have converged to the stationary distribution

```{r}
studytime_chains <- data.frame(studytime_sim[[1]])
quantile(time_chains$b.counts, probs = c(0.025,0.975))

# Summarise the posterior Markov chains
summary(studytime_sim)[1]
# Summarise the posterior Markov chains
summary(studytime_sim)[2]

ggplot(time_chains, aes(x = b.counts)) + geom_density() + 
  geom_vline(xintercept = quantile(time_chains$b.counts, probs = c(0.025,0.975)), color = 'red')
```

There is a positive association between time studied and grade achieved
For every 1 unit of increase in Study time, End of semester grade (G3) increases by 3.94
The 95% credible interval for End of semester grade (G3) is from 2.47 to 5.24 change per unit increase in study time.
